{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial_AutoSklearn_RemoteSensing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6zmHil4mIm2eZqmUnZKQa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nunocesarsa/CML_Short_tutorial/blob/master/AutoML/AutoSklearn/Tutorial_AutoSklearn_RemoteSensing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH5bySCszo6P"
      },
      "source": [
        "#Installing AutoSklearn\r\n",
        "\r\n",
        "- has a very hard installation... bear with me\r\n",
        "- distributed has to be removed and reinstalled\r\n",
        "- And that implies restarting the runtime \r\n",
        "\r\n",
        "https://automl.github.io/auto-sklearn/master/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jko0iyALEsYx"
      },
      "source": [
        "## Notice:\r\n",
        " you will have to restart the runtime (button appears in the cell) during the next cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3pWwn77bR9F"
      },
      "source": [
        "#it is a hard install on jupyter notebook\r\n",
        "!pip install PipelineProfiler\r\n",
        "\r\n",
        "!apt-get install swig -y\r\n",
        "!pip install Cython numpy\r\n",
        "!sudo apt-get install build-essential swig\r\n",
        "\r\n",
        "import distributed\r\n",
        "print(distributed.__version__)\r\n",
        "\r\n",
        "!pip uninstall distributed\r\n",
        "!pip install distributed\r\n",
        "\r\n",
        "import distributed\r\n",
        "print(distributed.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fJmQWSucA1E"
      },
      "source": [
        "!curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip install "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN2Y7Sm6cE6z"
      },
      "source": [
        "!pip install auto-sklearn==0.11.0 #weirdly this might require multiple attempts, something crashes in pyrfr or elsewhere. ALSO installing an earlier version because latest update crashes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHZSDpp4cele"
      },
      "source": [
        "#to investigate outputs of autosklearn\r\n",
        "import sklearn\r\n",
        "\r\n",
        "import autosklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrFSkORCdgQR"
      },
      "source": [
        "##Example "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C1VZBN1cxzD"
      },
      "source": [
        "#Example\r\n",
        "\r\n",
        "import autosklearn.classification\r\n",
        "import sklearn.model_selection\r\n",
        "import sklearn.datasets\r\n",
        "import sklearn.metrics\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "  X, y = sklearn.datasets.load_digits(return_X_y=True)\r\n",
        "  X_train, X_test, y_train, y_test = \\\r\n",
        "          sklearn.model_selection.train_test_split(X, y, random_state=1)\r\n",
        "  automl = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=60)\r\n",
        "  automl.fit(X_train, y_train)\r\n",
        "  y_hat = automl.predict(X_test)\r\n",
        "  print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XueodpandivR"
      },
      "source": [
        "### Exploring the pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oC-b6xAdWrK"
      },
      "source": [
        "import PipelineProfiler\r\n",
        "prof_data = PipelineProfiler.import_autosklearn(automl)\r\n",
        "PipelineProfiler.plot_pipeline_matrix(prof_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK_CTbW5m5Pt"
      },
      "source": [
        "# Setting up data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQs9bxl6h_hw"
      },
      "source": [
        "## Downloading\r\n",
        "\r\n",
        "The data is \"stolen\" from this excellent tutorial on how to do a classification with R:\r\n",
        "\r\n",
        "*   https://urbanspatial.github.io/classifying_satellite_imagery_in_R/\r\n",
        "*   https://github.com/urbanSpatial/classifying_satellite_imagery_in_R\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK6D_QpAh19v"
      },
      "source": [
        "! git clone https://github.com/urbanSpatial/classifying_satellite_imagery_in_R"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWchHumVl8KQ"
      },
      "source": [
        "## Exploring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFEZkXUvrc8Y"
      },
      "source": [
        "!pip install rasterio\r\n",
        "!pip install geopandas\r\n",
        "!pip install earthpy\r\n",
        "!pip install rasterstats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76zaVprul-bK"
      },
      "source": [
        "import os\r\n",
        "from glob import glob\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import rasterio as rio\r\n",
        "from rasterio.plot import plotting_extent\r\n",
        "import geopandas as gpd\r\n",
        "import earthpy as et\r\n",
        "import earthpy.spatial as es\r\n",
        "import earthpy.plot as ep\r\n",
        "\r\n",
        "import rasterstats\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrVgoC9z0OD4"
      },
      "source": [
        "## Checking out the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxuS2JWHr9jd"
      },
      "source": [
        "#From: https://earthpy.readthedocs.io/en/latest/gallery_vignettes/plot_rgb.html\r\n",
        "\r\n",
        "#fetches the iamges at 30m excluding the panchromatic and cirrus \r\n",
        "landsat_bands_data_path = \"/content/classifying_satellite_imagery_in_R/data/band*[1-7]*.tif\"\r\n",
        "\r\n",
        "stack_band_paths = glob(landsat_bands_data_path)\r\n",
        "stack_band_paths.sort()\r\n",
        "\r\n",
        "#resorting because of the band names \r\n",
        "stack_band_paths_sorted = [stack_band_paths[i] for i in [0,3,4,5,6,7,8]]\r\n",
        "\r\n",
        "#print(stack_band_paths)\r\n",
        "#print(stack_band_paths_sorted)\r\n",
        "\r\n",
        "# Create image stack and apply nodata value for Landsat\r\n",
        "arr_st, meta = es.stack(stack_band_paths_sorted, nodata=-9999)\r\n",
        "\r\n",
        "#From: https://earthpy.readthedocs.io/en/latest/gallery_vignettes/plot_rgb.html\r\n",
        "\r\n",
        "# Create figure with one plot\r\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\r\n",
        "\r\n",
        "ep.plot_rgb(arr_st, rgb=(3, 2, 1), ax=ax, title=\"Landsat 8 RGB Image\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaqkQUV40Kus"
      },
      "source": [
        "##Loading and ploting the points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MymaIaTz0Vks"
      },
      "source": [
        "#Loading and reprojecting\r\n",
        "sample_shp = gpd.read_file('/content/classifying_satellite_imagery_in_R/data/calgary_trainingPoints.shp')\r\n",
        "shp_prj = sample_shp.to_crs(epsg=32612)\r\n",
        "#shp_prj.crs\r\n",
        "shp_prj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F5uykMW-Q-e"
      },
      "source": [
        "#we have to fetch the extent using the rasterio package\r\n",
        "with rio.open('/content/classifying_satellite_imagery_in_R/data/band1.tif') as image_src:\r\n",
        "  img_data = image_src.read()\r\n",
        "\r\n",
        "  img_extent = plotting_extent(image_src)\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\r\n",
        "\r\n",
        "ep.plot_rgb(arr_st, rgb=(3, 2, 1), ax=ax, title=\"Landsat 8 RGB Image\",extent=(img_extent))\r\n",
        "shp_prj.plot(ax=ax)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaDlwQqUG-0j"
      },
      "source": [
        "#Extracting values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC_jF2XIM_PU"
      },
      "source": [
        "#first we save everything (the stacked raster and the projected shapefile to a directory)\r\n",
        "!mkdir outputs\r\n",
        "\r\n",
        "#saving\r\n",
        "shp_prj.to_file('/content/outputs/sample_loc.shp')\r\n",
        "es.stack(stack_band_paths_sorted, out_path='/content/outputs/Landsat.tif') #fails because of uint16 if we add the thermals\r\n",
        "\r\n",
        "#other alternatives here: https://gis.stackexchange.com/questions/223910/using-rasterio-or-gdal-to-stack-multiple-bands-without-using-subprocess-commands"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACwYMRXtr7S6"
      },
      "source": [
        "### Actual loop for extracting values... grrr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJVvKQHRSKbZ"
      },
      "source": [
        "from rasterstats import point_query\r\n",
        "\r\n",
        "for i in range(1,8):\r\n",
        "  #pt_query = point_query('/content/outputs/sample_loc.shp',i)\r\n",
        "  #print(pt_query[0])\r\n",
        "\r\n",
        "  #querying the data\r\n",
        "  pt_query = point_query('/content/outputs/sample_loc.shp','/content/outputs/Landsat.tif',band=i)\r\n",
        "  #print(i)\r\n",
        "  #print(pt_query[0])\r\n",
        "\r\n",
        "  band_nr = \"B\"+str(i)\r\n",
        "  print('Processing:',band_nr)\r\n",
        "\r\n",
        "  #adding to the pandas\r\n",
        "  shp_prj[band_nr]=pt_query"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQJNgQWsnhYm"
      },
      "source": [
        "# Setting up for Autosklearn\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cifs8XsKrN9Q"
      },
      "source": [
        "### Partitioning the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNEsgl0er0l0"
      },
      "source": [
        "shp_prj\r\n",
        "#notice the units of the bands should be converted to reflectance but it doesnt really matter for machine learning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeXINJHcrP_m"
      },
      "source": [
        "#removing NA from the datasets\r\n",
        "test_data = shp_prj\r\n",
        "test_data = test_data.dropna(0) #there is one row somewhere with an NA that causes errors, se we just remove it\r\n",
        "\r\n",
        "#selecting data and conerting to category\r\n",
        "df_x = test_data[[\"B1\",\"B2\",\"B3\",\"B4\",\"B5\",\"B6\",\"B7\"]]\r\n",
        "df_y = test_data[['class']].astype('category')\r\n",
        "\r\n",
        "#using Sklearn data splitter\r\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(df_x, df_y, random_state=42,test_size=0.3) #.7/.3 split"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDTTc9dFsUCJ"
      },
      "source": [
        "#feel free to explore the datasets here\r\n",
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okBrPc6ueeIj"
      },
      "source": [
        "## Applying AutoSklearn to the RS data\r\n",
        "\r\n",
        "Find more options for Autosklearn @ the API page: https://automl.github.io/auto-sklearn/master/api.html\r\n",
        "\r\n",
        "\r\n",
        "- Autosklearn can be run using a \"meta-learning\" approach: meaning, the first N models tested are a set of pre-trained pipelines tested on other data which are \"similar\" to the current dataset\r\n",
        "\r\n",
        "- \"Simulates\" the expertise of a user who would have \"seen data that looks like this before\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8fVkRiOgG6H"
      },
      "source": [
        "time_spent = 60 #total time spent trying different models, the more time, the more models are tested\r\n",
        "\r\n",
        "#this command constructs the object auto_mdl, an autosklearn insteance\r\n",
        "auto_mdl = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=time_spent,\r\n",
        "                                                       resampling_strategy='cv',\r\n",
        "                                                       resampling_strategy_arguments= {'folds': 5},\r\n",
        "                                                       seed=42,\r\n",
        "                                                       n_jobs=-1)\r\n",
        "\r\n",
        "#this command actually performs the fit\r\n",
        "auto_mdl.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJAY_Rj6k0qa"
      },
      "source": [
        "y_hat = auto_mdl.predict(X_train)\r\n",
        "print(\"Training accuracy score\", sklearn.metrics.accuracy_score(y_train, y_hat))\r\n",
        "\r\n",
        "y_hat_vl = auto_mdl.predict(X_test)\r\n",
        "print(\"Validation accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat_vl))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgVqsndSh7rJ"
      },
      "source": [
        "## Investigating the pipeline:\r\n",
        "\r\n",
        "- this can be used to identify \"common\" pre-processing steps that improve the accuracy\r\n",
        "- Can also be used to identify specific algorithms that work better \r\n",
        "- And naturally, all this knowledge can be encoded to a custom autosklearn instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjP9CaiQh6Ef"
      },
      "source": [
        "prof_data = PipelineProfiler.import_autosklearn(auto_mdl)\r\n",
        "PipelineProfiler.plot_pipeline_matrix(prof_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0yHjmfPi_Sh"
      },
      "source": [
        "#Predicting on a raster\r\n",
        "\r\n",
        "- I reckon this can be simplified but i adapted from a crappy code i used for testing in another case\r\n",
        "\r\n",
        "1.   List item\r\n",
        "2.   List item\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZduhyvn4TOV"
      },
      "source": [
        "## Preparing the data\r\n",
        "\r\n",
        "- In general: Convert the multy-layer raster object to a \"table format\", apply the model and reconstruct the raster. \r\n",
        "\r\n",
        "- (Optional) This procedure above allows me to process in data chunks and to avoid going out of memory on the google colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrdynTKcvcz8",
        "outputId": "9793a119-ce95-40eb-eec0-9807123e14ce"
      },
      "source": [
        "!pip install pyrsgis\r\n",
        "from pyrsgis.convert import changeDimension\r\n",
        "from pyrsgis import raster\r\n",
        "import rasterio\r\n",
        "import pyproj\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "\r\n",
        "#lading the landsat data \r\n",
        "ds1, bands = raster.read('/content/outputs/Landsat.tif')\r\n",
        "\r\n",
        "print(ds1)\r\n",
        "print(bands.shape) "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyrsgis in /usr/local/lib/python3.6/dist-packages (0.3.3)\n",
            "<pyrsgis.raster.createDS object at 0x7f5029042630>\n",
            "(7, 1413, 1121)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoZ0Eh2lwo6i"
      },
      "source": [
        "#creates a np with 7 columns anx NxM rows (and also its tranposed)\r\n",
        "\r\n",
        "bandByPixel = changeDimension(bands) #we have to devide all values by 10k - its a conversion from bits to reflectances\r\n",
        "bandByPixel_t = np.transpose(bandByPixel)\r\n",
        "\r\n",
        "print(bandByPixel.shape)\r\n",
        "print(bandByPixel_t.shape)\r\n",
        "\r\n",
        "pd_bandByPixel = pd.DataFrame(data=bandByPixel,columns=[\"B1\",\"B2\",\"B3\",\"B4\",\"B5\",\"B6\",\"B7\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yctzFqznv1qE"
      },
      "source": [
        "pd_bandByPixel #notice that many of the lines are 0, because, the original raster is padded by 0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lrz2UCd4QHN"
      },
      "source": [
        "## Prediction time\r\n",
        "\r\n",
        "- be ready to wait a bit... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf3q23UY4IqD"
      },
      "source": [
        "y_pred = auto_mdl.predict(pd_bandByPixel)\r\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX_tpazf5Ek9"
      },
      "source": [
        "## Rebuilding the raster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5nrtreb8iJb"
      },
      "source": [
        "## Converting \"strings\" to numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z26pcBs29Dq6"
      },
      "source": [
        "#checking the clases\r\n",
        "shp_prj['class'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4Em_WF29Gxu"
      },
      "source": [
        "#lazy but i don't care:\r\n",
        "y_pred_num = np.where(y_pred=='water', 0, y_pred) \r\n",
        "y_pred_num = np.where(y_pred_num=='undeveloped', 1, y_pred_num) \r\n",
        "y_pred_num = np.where(y_pred_num=='developed', 2, y_pred_num) \r\n",
        "y_pred_num = np.where(y_pred_num=='clouds', 3, y_pred_num) "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EsOv5oN5GzT"
      },
      "source": [
        "#rebuilds the raster from the table we used before\r\n",
        "y_pred_np = np.reshape(y_pred_num,(ds1.RasterYSize,ds1.RasterXSize))\r\n",
        "y_pred_np.shape\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_6E8glt9_Th"
      },
      "source": [
        "##Saving the raster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D8UMb947WCs"
      },
      "source": [
        "#exports the data to our work space\r\n",
        "raster.export(y_pred_np, ds1, '/content/outputs/Landsat_Class.tif', dtype='int') "
      ],
      "execution_count": 62,
      "outputs": []
    }
  ]
}